[
["index.html", "Data Science for Petroleum Production Engineers Preface", " Data Science for Petroleum Production Engineers Alfonso R. Reyes v 0.0.5 Preface This is the first draft of the book Data Science for Petroleum Production Engineering. It presents a collection of algorithms, datasets, solutions on practical challenges in the uptream side of the production cycle of oil and gas. We use modern tools; data science tools such as Python and R to help us gain a quick understanding of the production data. "],
["data-science-for-petroleum-production-engineering-part-1.html", "1 Data Science for Petroleum Production Engineering (part 1) 1.1 The Old vs the New 1.2 Do worksheets, spreadsheets makes you dumb? 1.3 Why does it take so long to build optimization models? 1.4 The cheapest oil you can find? 1.5 Data structures are our friends 1.6 Remember: Garbage In, Garbage Out 1.7 Digital Notebooks are a fit for Engineers 1.8 Python or R", " 1 Data Science for Petroleum Production Engineering (part 1) 1.1 The Old vs the New In the last century, the production engineer built the well models one by one and analyzed the results also one by one. With the ubiquity of the personal computer, desktops and laptops, an unimaginable computational power has been put in our hands. So, why takes so long to update well models? Why still are we doing it one-by-one? Why -if nowadays we transfer data at gigabits per second-, are we still using tools of the era of bits per second? 1.2 Do worksheets, spreadsheets makes you dumb? The spreadsheet was invented in the 80’s and was a great invention. The beauty of it is that you can produce results right away. The cells model has been so nice to us. We are very thankful to Lotus 1-2-3 and Microsoft Excel for that. But let us remember that it was the 80’s –there was not internet, no tablets, no smartphones, no laptops. Data moved at 2400 bps (or even less), and real-time data acquisition was constrained to a handful of fields with luxury mainframes. Today, well data comes in gigabytes per minute and we know it is not humanly possible to tackle all the information to put it at good use. The answer of this century is to that is start using automation, artificial intelligence, statistics, machine learning but first of all, data science. These are the keywords of the future. The future of the petroleum engineer. The thing is: there are not smart wells or smart fields, without smart tools and smart people and without modern computational statistics 1.3 Why does it take so long to build optimization models? One of the things that always intrigued the production managers what is it that it takes so long in building well models and their corresponding network models? One of the causes is that the production engineers have to enter the data manually; have to double and triple check the data being entered; calibrate and match the model; and – the most important of all-, entering the data without a context, well by well. What does that mean? It means we work on a well model in isolation without having the tools to validate our well data in the context of the field data or of other well data (neighbors). Then, when we have all our 77 or 177 wells ready, we received fresh well test data that contradicts earlier well tests! Go back to rework the models. Part of this conundrum is not the production engineer’s fault; it is just the way that all well optimization software works or has been built: to attack a well one at a time and disconnected from the test data source or from other wells. Some companies have been using for a while Python scripts to help to address the problem of single well modeling in a radical different way: by building well models with multiwell scanning, quality control, validation and analysis, before and after the model has been built. With these techniques the engineer minimizes the manual data entering, validation, calibration and data qualification. This by itself represents a major advantage over the traditional way of doing well and network modeling and optimization. The additional power and discovery of oil gains from production optimization comes from applying basic statistics to: the well input data: quasi-static (PVT, completion, deviation survey) and dynamic (well tests, artificial lift data) nodal analysis oresults f the unconnected well results or calculations of the network simulation runs 1.4 The cheapest oil you can find? The plots that you see in this post are just few samples of the work of well and network modeling on a field. Field names and well numbers have been anonymized to make the exercise neutral. None of this findings is earth shattering new; it has been around for years but the digital tools that are making high-tech companies successful are taking a little time to take off in the petroleum production engineering field. Well, first, data science is hard; statistics is harder. But enormously gratifying when you find hidden oil. Oil found with statistics and data science is the cheapest oil you can get. 1.5 Data structures are our friends One of the keys in data science is to get accustomed to analyze well production data by using dynamic tables. There is no best tool for analyzing production engineering data than dataframes or data tables in Python or R. The pandas tables (yes, it is written in lowercase) are magnificent objects to manipulate, arrange, organize, calculate, clean, select, filter, analyze and plotting production data. Engineering libraries for the Production Technologist can be created to include scripts that make use of production datasets with the help of packages such as numpy, scipy, matplotlib, PyQt4, pyqtgraph, bokeh, traits, etc., in the Python environment. With the computational libraries the engineer is able to analyze the well models data faster and in a more reliably way since all these tasks are done by digital algorithms, using scripting languages for scientists and engineers. And, best of all, it is open source. Your are curious? This is how a simple statement looks like in Python: From the data we can tell that the biggest producers are wells producing by gas lift with an average of 944 blpd (barrels of liquid per day), a maximum of 2421 blpd in a total of 98 wells. The lowest producer is at 100 blpd. 1.6 Remember: Garbage In, Garbage Out In next posts I will be showing simple Python and R scripts for petroleum production engineering. What we see in the next plot is a plotting technique for quality control of the well test data to be used in the well models. Before we run the models it is recommended that you run some scripts to perform some quality control on the data such as finding outliers or data points that do not belong there, or even addressing missing data. You will also observe in some examples how convenient is scanning stand-alone well models with Python scripts to detect outliers in well parameters such as: reservoir temperature, water salinity, gas specific gravity, API, injection gas data, GOR, pressures, etc. By refining these technique we will see later that we can even do quaility control on the results of the nodal analysis phase to calibrate and match the models to the well test data. 1.7 Digital Notebooks are a fit for Engineers The Python notebooks look ideal for the application of data science to well, network modeling and production optimization. There are many ways of running Python but I find the notebook to be the best method to transmit knowledge and share solutions with colleagues. The Python notebooks -lately renamed as Jupyter -, are widely known in the engineering community. You shouldn’t have any trouble finding assistance online. The same applies to Python. My favorite place in the web to find help is stackoverflow.com. There are hundreds of engineering and scientific libraries around that will make your production engineering job more efficient, accurate, profitable and fun. And of course, faster. The Python notebook opens in a web browser but if you want to make long and advanced scripts for petroleum engineering, there are plenty of good development applications or IDEs out there: Spyder, PyCharm or Eclipse, just to mention a few. 1.8 Python or R You have seen in other posts that I fell in love with R. But Python has not be forgotten. Both cover different needs and have their own space of application. Python is an scripting language that is easy to learn, very much similar to the human natural language, it has thousands of libraries that cover all sthe spectrum of industries and applied sciences. R focus is more on statistics. Let me rephrased that: “advanced statistics”. Both languages deal with data in their own style. While Python is a general purpose language, R strength is in resolving problem using vectorial abstractions. Both are beautiful, both are high level, and both fill different user needs. They are not competing but sharing the same information super highway. "],
["dsppe-acquiring-the-data-part-2.html", "2 DSPPE - Acquiring the Data (part 2) 2.1 An engineering library 2.2 Multiwell Scanning 2.3 Data frames and data tables 2.4 The Excel way :( 2.5 Scripting in Python 2.6 Loading the dataset 2.7 Plotting 2.8 What are we learning so far?", " 2 DSPPE - Acquiring the Data (part 2) 2.1 An engineering library In the past session we got an introduction to the multiwell statistics package showing a few of the things that we can do with an engineering library implemented with Python. Now, we will explore some more functionality. It is incredible the huge amount of information when we get from all the wells in one scan pass. The well data starts to have sense. A well in isolation or standalone doesn’t tell used much about the field or differences between well parameters from well to well. This is where the power of the multiwell scanning scrip resides. 2.2 Multiwell Scanning The multiwell scanning is the process by which we use a Python script to automatically open all the well models in the background, pulling the user-defined parameters and create an output file where the rows are the wells and the well parameters are the columns. In data science parlance, the rows are called observations and the columns are the variables. The scanning open the optimization software, which could be Petex’s Prosper, AppSmiths’ WinGLUE or any other application that has a way to share its variables and control to the outside. In the case of Petronas, we adopted as a standard both. In this example I will use use Prosper. This is how a file with the input variables look like: These are few of the variables that we want to pull from each of the well models to be scanned. Note that the first 7 columns belong to the decomposition of the Petex OpenServer variables. I created this input file to make possible a repeatable selection process. In one of the datasets we will see 94 variable-columns; in other maybe less than 40. You can also have a set of input workbooks; one for PVT analysis, another for well test, or gas lift, or IPR or VLP selection. In another post I will enter in detail about this input worksheet. The interesting thing to note here is that we can select what well variables we will be extracted from each of the well models and written to the dataset. You can do this to 10 well, 50 wells, 100 or 1000 wells. You can gain great insights from from comparing all the well models at once. Soon it will become evident which is one the IPR correlation most used in the company, what PVT correlations give a better matching or which VLP adjust better to the well or field. 2.3 Data frames and data tables After the script finishes gathering data from the last well you will end up with an output file. The tabular form of the data in that output file is what is called a data frame or a data table. The table in our example will like this: 2.4 The Excel way :( Yes, it is Excel. But we just use it as a legacy tool so we can share the results with any production technologist without much complication. Some have questioned why we do all this data processing with Python instead of using Excel. The question is very simple, and I think you will quickly understand if you have ever worked with datasets in Excel. I will just cite few of the inconveniences: the macros VBA are alright if you are writing less than a hundred of lines of code; the code in VBA is difficult to maintain and there is no version control like with Git or similar. Version control is essential in dealing with data analysis. Microsoft VBA has its own way of doing things and you have to learn a lot about the internals of the MS libraries if you want to do little above of the ordinary calculations and plotting; VBA code is difficult to share with colleagues and when you make a change or update the code you have to send again a big workbook, in other words there is no updating process embedded within Excel and VBA to load new code. Reproducibility Antiquated plotting Don’t get me wrong. I have done terrific things with Excel VBA but it is painful by the lack of an engineering or scientific community behind as is the case with Python or R. Let’s say that Excel is good at table visualization that anybody can open, understand it and do some basic plotting around it. But as you get immersed with Python and pandas you will see that using Excel as data containers is purely optional; there are other powerful data structures available out there. 2.5 Scripting in Python The Python script that open the well models and pulls the data is this: Each of the lines in the script is explained through a comment above. The comments come always after the pound # sign. Let’s briefly explain what this script does. What we are trying to achieve is reading all the Prosper well models located under the folder PISC. You are interested in reading some variables from these models and putting them in a table-like structure for statistical analysis later. These variables have been defined in the input workbook –explained above. 2.6 Loading the dataset Now, you have generated yourself a dataset. This dataset contains the data of 100 wells that was just generated by the application multiwell scanner. Let’s load the dataset with this script: Immediately with the execution we get some information about the well collection: 2.7 Plotting To start plotting, we can just write: For artificial lift: For the IPR methods that were used on each of the well models: 0 And for the PVT correlations: 2.8 What are we learning so far? At this moment you may be wondering: kind of well modeling is hard; it takes a lot of work. So many variables to take care of. Yes, it is. That’s why getting a network model ready for scenarios and simulation takes so long. Now, imagine if you have to do this analysis by hand, or using the mouse and clicks. The advantage here, with the Python scripting library, is that we are acting on multiple wells; not just one. There is a significant reduction in time and improvement in the quality of the data by tackling the problem in this way. But requires a toolbox: the toolbox of the data scientist. You are now using the bleeding edge of the digital tools available to the brightest minds in the world to find more oil; faster and efficiently. TODO * add conclusions * maybe more plots * add code and links to code "],
["dsp2e-the-well-network-dataset-part-3.html", "3 \\(DSP^2E\\) - The Well Network Dataset (part 3) 3.1 Scanning well datasets 3.2 The Network Model 3.3 The need of automation for well network modeling 3.4 The network model dataset 3.5 Reading the model variables with automation 3.6 Gathering auxiliary data 3.7 The field view 3.8 The KPIs dataset for Gas Lift 3.9 Other datasets 3.10 Basic statistics 3.11 Advanced statistics 3.12 Conclusions", " 3 \\(DSP^2E\\) - The Well Network Dataset (part 3) 3.1 Scanning well datasets In our last post (Part 2) we reviewed the multiwell dataset generated by scanning multiple well models built with the software Prosper. This time we will analyze a network model tha has been built with GAP - the well network analysis tool from Petroleum Experts -, using well models that have been previously calibrated and matched. The results have been imported to a dataset using scripts. TODO - [ ] share the multiwell dataset in Github - [ ] create an email and user name for Oil Gains account in Github\\ - [ ] Or later, we promise to provide the links? Typically, these network models are analyzed by viewing the results graphically inside GAP and in the built-in solver table or exporting the text data to continue the comparative analysis with any office application. This represents a little of an inconvenience because: the results cannot be compared to other scenarios unless you take manual notes; or copy-paste it somewhere else. no graphical analysis can be directly performed because the application lacks plotting capabilities for the solver output; the data that we need exported is not structured or tidy, -meaning, lacks columnar format or something resembling data tables or data frames. This requires time of the engineer for parsing and format the solution table from GAP to be understood by Excel or other analysis application; lastly, the solutions are not saved in a historical file for future reference; the solutions from the solver are lost on time. In some way we have to deal with it. We cannot expect from sophisticated state-of-the-art calculation engines to be full of features to satisfy the data scientist in our production engineering department. We understand we are in a transition to an era of big data. We will get there soon. We expect that petroleum engineering software will catch up with the times. In the meantime, we can handle our data analysis with Python, R or other data science analysis tools available. The objective of this article is sharing an experience on applying data science to the results coming out of network modeling work on a set of oil wells. These oil wells are combination of flowing and gas lift wells; mostly gas lift wells. We are looking to optimize the production schema of the system by finding the right tubing head pressure, gas lift injection rate, gas lift injection pressure, choke size, separator pressure and other factors. 3.2 The Network Model As we mentioned above, the export format of the calculation results of network modeling for well optimization is non-structured text but still it can be printed or pasted directly to Excel for parsing to separate the data in columns (variables) and rows (observations). In some way, this is time consuming and a potential source of errors. Worse if several iterations have to be performed on the well and network input data for the generation of optimization scenarios. Certainly, this process is screaming to be automated. But that takes time and resources. 3.3 The need of automation for well network modeling This is what this is about: generating a repeatable and automated script to read the network model from the original source or calculation engine: GAP. GAP is a comprehensive application from Petroleum Experts to resolve well networks. It is unique in the sense that counts with a very powerful interface for data interchange and control with other applications; it is called OpenServer. With OpenServer it is possible to read and write variables from and to the network model. Also allows the production engineer to customize production optimization scenarios in the most complex fashion making possible an automated process by means of iterative input to find a solution that better adapts to the conditions of our field. In this article we dedicate our attention to the GAP Solver that looks like in the figure. It corresponds to a generic example. ￼] Figure 1 - GAP solver panel from one of the examples. 3.4 The network model dataset The well network dataset is formed essentially by the columns in the solver. In the figure above you see on the first column various items that are part of the network such as flowlines, separators, joints, etc., but we are looking for is only the wells. Of course, you can analyze other network components but that could be a topic of a different article. Each of the columns in the table can be extracted, as we explained above, by copy-pasting to Excel, or by using OpenServer. We read each of the columns (the ones that we are most interested in) and create a dataset. We will explain later how this reading process works (involves Python and OpenServer). [ ] show obfuscated datase/table/dataframe The rows in the dataset correspond to the wells and the columns to the variables that were extracted from the columns in the solver. Instinctively, we may tempted to say, after seeing the solver table, that the dataset should be just the calculated columns in that table. But that would be an incomplete dataset. Besides of the solution for the scenario proposed, we require additional data for the network of wells under analysis. -[ ] what is the additional data? 3.5 Reading the model variables with automation Reading the GAP model requires a connection to OpenServer through a COM server/client connection. In our example, GAP is the server. There are few examples under the OpenServer folder in the IPM examples installation folder that uses Excel VBA. For simple examples, Excel is fine. For serious programming and more complex tasks it is recommended to use Python or other high level language. We show below a pseudo-code explaining the steps for reading GAP variables. Writing to GAP is similar. TODO - [ ] explain the GAP variable - [ ] show few examples of the variables: GAP.well[0].bla-bla-bla - [ ] show a basic code to read the variables to a dataframe. Pseudo-code - [ ] link to the code # pseudo-code for GAP automation script with Python from ptech import * connect_to_openserver() start_gap() select_network_model() open_network_model() wells = NetworkModel(wells) data = list() for well in wells: data.append(read_variables(well)) create_dataframe(data) shutdown_gap() close_openserver() 3.6 Gathering auxiliary data Collecting the data for the network model is not a major problem; most of the data is coming from the well models that we saw in Part 2 (add link). At this stage, the well models have been matched to their IPR (inflow performance relationship), VLP**(vertical lift performance), and calibrated against their latest well test and FGS (flowing gradient surveys). Additional data for the network model such as constraints, wells on-line, choke orifice size, flowline dimensions, separator pressures, total field production are entered directly in the model in GAP. TODO what else? 3.7 The field view During the construction of the network model, conditions change: wells that are on-line, wells that are shut-in, compressors down, flowlines opened or close or bypassed, changes in the chokes, back-pressure, etc. When we run the network optimization we are looking for a way to visualize in one plot the current status as is in the model file in GAP. This saves considerable time in navigating through screens and checking the well condition one by one. What we see in Figure 2 is a customized plot that was written using matplotlib, a Python plotting library. The variables shown are the minimal that could be shown as critical for the verification of the model. Figure 2 - The well network summary plot. Each of the rectangles represent a platform. The wells in green are the ones active or producing. The well with suffix NF, GP and GL mean that that well is a natural flow well or it is a gas producer or it is a gas lift well. At the bottom of the rectangles we see the total number of strings (or wells). The letter “E” below means the number of wells that are enabled in the model, while “D” indicates how many wells were disabled in the network model. [ ] change the plot above to something neutral 3.8 The KPIs dataset for Gas Lift Since most of the wells in the dataset were producing with gas lift, In order to find relationships between the different gas lift parameters we build an additional dataset based on common gas lift formulas in Figure 3 to evaluate the gas lift performance of the field. The new dataset uses as a base for the calculations the columns available in the solver dataset. ￼ Figure 3: Key Performance Indicators (KPIs) for gas lift The reason for doing this is to observe the gas lift indicators under the light of the solutions recommended by the network optimizer. We try to find out if there is a better scenario that just maximizing the oil production, such as reallocating gas or give preference to wells with a lower watercut. This is how the gas lift KPIs dataset looks after applying the formulas. ￼ Figure 4 - a dataset generated from the solver solution. TODO - [ ] obfuscate data. Note that we have kept the well name as a column to identify the row or observation. 3.9 Other datasets [ ] mention what are the other datasets that will be published 3.10 Basic statistics ￼ 3.11 Advanced statistics TODO * round the tables ￼ ￼ 3.12 Conclusions There is always a way to get an engineering solution to be statistically analyzed. Use plots to visualize a summary of the data. Don’t be timid at generating additional datasets from the solution data. Look at what the plots tell you. They always will try to convey something. You are not wrong if you underestimate Excel as a potting tool. Look outside for better plotting tools but manage first how to get the data out. There is more gains to be made by analyzing your solution outside your favorite engineering software. There is no such thing as a software that does everything for everyone“. "],
["r-and-the-search-of-the-ideal-language-for-petroleum-engineering.html", "4 “R” and the search of the ideal language for petroleum engineering 4.1 The fun of the 21st century 4.2 What is R? 4.3 Ploting 4.4 Packages 4.5 Foreign languages 4.6 Reproducible 4.7 GUI not 4.8 Where is everybody 4.9 The bottom line", " 4 “R” and the search of the ideal language for petroleum engineering img 4.1 The fun of the 21st century I have to confess my sin. I am coming from writing thousands of lines of Python code for a well modeling and optimization project. I did Exploratory Data Analysis for the resulting well models and applied statistics using Python, pandas, matplotlib, SciPy and NumPy. I went full throttle and didn’t look behind. Got the results, the production engineering team increased the oil production we were looking for. All went fine, with the exception of the sin of not exploring the other alternative: doing the work with “R”. If you haven’t heard about R you are missing the fun of the 21st century: data science. I might not be totally to blame though. Python is a relatively easy to learn scripting language, fast prototyping applications and producing quick results. Also, it has under its domain pretty good engineering and scientific tools. Like the ones I cited above. Plus the statistical packages that have been flourishing over the years. 4.2 What is R? On the other hand, there is R, another programming language with the peculiar feature of having been invented by statisticians about couple of decades ago. R is the preferred open-source language when it comes to publishing research, doctoral papers, statistical analysis, supervised and unsupervised machine learning, regression analysis, forecasting, exploratory data analysis, multivariate plotting or visualization. R can be intimidating. Didn’t I tell you that its users are mostly PhDs? R is different of any other language I have seen over the years. And that comes, I believe, from its intense focus on the data, data analysis, statistics and vector oriented operations. After few months spent on learning and working with R, I can tell you that any sweat was worth every penny. It is not only the language itself but the whole R ecosystem that surrounds it: the data types abstraction; the fact that can be run in interactive mode from a console, or by scripting automatic operations; the enormous plotting capabilities; its solid library of packages that always runs; that can easily integrate with lower level languages such as C, C++ or Fortran. R practically reads and writes any possible data format or database. 4.3 Ploting Plotting. The plotting capability is embedded in the base system itself. One of the major strengths is visualizing the data to make sense when there are millions and millions of numbers. No other language gets closer to that power. If you have tried C++, Python and Java you will have to end up searching for libraries and still will have to do a lot of leg work to get your plot right. Not in R. Besides the base plotting there are other powerful packages that even add a grammar of graphics such as ggplot2, which make R unique. 4.4 Packages Packages. Publishing a package in R, sort of has become a badge of honor due to the stringent requisites and numerous rules you have to comply with to make your package available to its repository CRAN. And I think for a good reason, R has kept kept solidly growing and giving reliable results. And it is on the subject of reliability where the major separation between R and Python starts. If there is something that the Python community has to do to is improving the reliability of the code. Maybe by centralizing the package development under more rigorous standards. Today, there is not only two major Python versions out there competing, 2.7 and 3.5, but package compatibility varies from distributor to distributor. An application developed using Python vendor X will not run without hiccups or crashing in A or E or vice versa. 4.5 Foreign languages Speaks C, C++ and Fortran. I think this is the one of the characteristics that caught more of my attention during my R exploration. If there is a challenge in engineering, it is to make the applications faster and more efficient. C++ and Fortran bring that besides the enormous baggage of scientific libraries; proven and tested. I tried it with calls from R to C, C++ and Fortran and they just work without touching your settings in the operating system. I tried in Windows, Linux and Mac OSX, and all of them compile without needing an IT guy or Admin on your side. R makes an amazing engineering tool. I even tested calling math libraries in Java. 4.6 Reproducible Reproducible. This is a concept we may not have seen often in the petroleum industry. And that is possible due to our extreme reliance on spreadsheets. Everybody likes getting quick results. Don’t get me wrong. But the same characteristic of formulas and cells that made the worksheet the king, it is at the same time, its weakness. You cannot replicate safely the results unless you look at and revise cell by cell. You will never be sure that nothing has been improperly copied and pasted. And that’s a big problem. Spreadsheets nurtures bad data habits. I think it is here where the great influence of biomedicine, biogenetics, biogenomics and similar branches of the bio sciences play a vital role because they cannot afford any mistakes. The work of a researcher has to be able to be reproduced by colleagues or third parties before publication of the conclusions. The whole R environment has been molded in that way. The results should be the same -based on the same data-, regardless of who was the person running the analysis. Besides the benefit of repeating it in the future leaves lessons for others to learn. 4.7 GUI not No need for GUI. It must be the data-centric nature of R, and its focus on the statistics, that doesn’t make you miss a graphical user interface. So, let’s say you are tremendously happy that you can produce your analysis, plots and report that you forget that you need a window with buttons, dialogs or menus. You are busy with the data, the calculations, the algorithms and the statistics without paying attention to the bells and whistles of a fancy user interface (UI). The folks from RStudio -one of the R vendors-, came up with a beautiful model for the graphical interface, browser based, and its name is Shiny. I heard about it. Got worried: here comes another HTML, JavaScript, CSS tool to learn. Nope! Wrong! They have made possible an abstraction of the whole graphical construction to just simple use of R code. I remember producing my first graphical statistical application, with sliders and all, in approximately 5 minutes. It’s easy even if you don’t know much R; like I was few months ago. img 4.8 Where is everybody Why not everybody is using R then? R is hard. It looks different because it does things differently. I remember taking a look at my first book of R and thinking “rather do then my project in Python …” R exhibits a unique level of abstraction when it comes to the way of manipulating the data; it doesn’t resembles much of Python, or much less of Visual Basic. The best and most efficient operations are performed using vectors and it is heavily discouraged -but not forbidden- to use loops to perform iterations. Rather be using apply, lapply, sapply or the other apply functions. Getting out of the loop - no pun intended - possibly, is one of bad habits harder to break. It takes a while to get accustomed to the functional programming demands of coding in **R. But it is neater and one gets amazed that one line of code can do so much to summarize a huge data table and replace nested “for” loops. 4.9 The bottom line The bottom line. R, and the other scripting languages available, will always make your engineering and data science results better, state-of-art, reproducible, shareable, version-controlled, with publishing quality and with much, much better plots than the canned worksheet graphics, which by the way, haven’t changed much for the past 25 years. You can follow me via Twitter via fonzie@OilGains "],
["how-big-is-your-data.html", "5 How Big is your Data 5.1 Everything is relative 5.2 How big is big 5.3 Tools for every size", " 5 How Big is your Data 5.1 Everything is relative 5.2 How big is big 5.3 Tools for every size Why is it called big data? "],
["thp-bhp-matching-from-pdg-data.html", "6 THP-BHP matching from PDG data 6.1 Matching is not forcing 6.2 DP plot 6.3 It is not only the VLP curve / Don’t blame the tubing performance curve 6.4 Closing", " 6 THP-BHP matching from PDG data TODO * Add small introduction or abstract for the article 6.1 Matching is not forcing we have to be careful with what the term “matching” means and represents. First of all, what matching is NOT is forcing the calculated BHP (\\(BHP_c\\)) from the VLP to be the same as your measured BHP (\\(BHP_m\\)). Unfortunately, this is how generally is interpreted because of how easy the software allows you do do just that. But again, forcing or fudging whatever the the well data or parameters the software allows you to tweak to match the calculated BHP with the measured BHP doesn’t mean you are matching the model. Let me tell you this: matching a well is not a one-off task; it is a process. It starts with some common sense first. And that has to do mainly with the data. If you take this well modeling as a task your matching would only last just one day: the day that your forced the matching with one data point. That is not a well model. A well model is a mathematical or computational representation of your real well, which mean that the IPR, VLP, PVT, temperature correlations or mechanistic models have to represent your real well with some level of accuracy. Not for one day but for a longer period of time. TODO * add well data to reproduce this plot * can it be a real well? * R or Python? vlp-matching-before-tuning 6.2 DP plot To show you what I mean by that let’s look at the depth vs. pressure plot (PD) or flowing gradient for any VLP model. Let’s call them \\(K_T\\) to the real VLP that better follows your well behavior; \\(K_A\\) to the VLP correlation that you selected. From the figure we can see that the \\(BHP_c\\) exceeds the measured pressure or \\(BHP_m\\) from your permanent downhole gauge (PDG). There will be many things in the software that you can tune to force your \\(BHP_c\\) to match the \\(BHP_m\\). One of them is to switch the VLP correlation or mechanistic model that better follows your pressure point. So, the first question is: does this VLP model really represents your well or your field? How long has this correlation being used? Is there a good history of this VLP matching other wells in the neighborhood? So, you can see how easily you can change between VLP models from 10 to 1 available, right? But if you do that this is how you could end up: vlp-matching-after In the IPR/VLP figure you match the \\(BHP_m\\) but your production rate doesn’t make sense anymore. Why? Because if you are forcing the match of the BHP is finding a point on the horizontal pressure line. Now the pressure matches but the flow rate could be lower or higher than the real production. And this is just one of the many cases. This is not matching; it is forcing a match. Avoid this path. 6.3 It is not only the VLP curve / Don’t blame the tubing performance curve There are few things that should be checked first; it is not just the VLP model selection. These are few that I have seen to have an effect on the VLP. These other well parameters may cause a considerable effect on the behavior of the VLP curve but are usually disregarded or not paid enough attention: TODO * show the effect with a dataset * make a sensitivity plot * Use R PVT data. The bubble point, GOR and selection of the PVT correlations. The wellhead temperature and the HTC (heat transfer coefficient). If these are not correct, your model may not even run or give you weird values. You will be surprised that not many read the wellhead temperature. The completion design properly entered. This is not often paid careful attention: the ID of the tubing, change of diameters, depth of a valve. And guess what? The downhole equipment does not even represent what you have in the well; an outdated well schematic. If you are injecting gas, the density and composition of that gas could make nearly impossible to perform any matching. Same with the valve data, orifice size, injection point, injection rate, free gas and solution gas. How reliable your IPR curve. The less data you used to define your IPR, then the less accurate your IPR model will be, therefore, the less likely your flow rate will match your BHP. Not all wells or field are so lucky of having a permanent downhole gauge. If you have it, then this outs you in an advantageous position compared to others that can barely afford a flowing or static gradient survey every two or three years. The well test production data. Very important to cross-validate your BHP measurements. Both can be used to perform your model matching. I am talking about the production rate, tubing head pressure, watercut and GOR. You cannot expect 5% accuracy in a model when you have 20% uncertainties in the value of the GOR or 15% in the THP. 6.4 Closing One more thing. The standard practice of resolving well models on a well by well basis, the current paradigm, will take longer and most likely give inaccurate results. Think about doing well modeling under the context of all the wells in the field and observing all the well parameters of the well neighbors under study. You will see dramatic improvements. When we analyze the well data under a wider context, like the field, then the well parameters start making sense. One value, like wellhead temperature or wellhead pressure or reservoir temperature alone, in one well by itself, may misled us if it not compared with its peer wells. The temperatures or pressures may be out of range in a standalone study but if we include in the study the other wells, then we can expect better results. I am attaching a couple of figures that shows one of the cases of the effect of a VLP correlation forced to match a downhole pressure. There are more, of course. TODO * Is there a way to show an schematic of both approaches? "]
]
